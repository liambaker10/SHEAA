{% extends "base.html" %} {% block title %}About Models{% endblock %} {% block
content%}
<html>
  <div id="main">
    <link
      rel="stylesheet"
      href="{{ url_for('static', filename='css/style.css') }}"
    />
    <!-- One -->
    <section id="one">
      <div class="inner">
        <header class="major">
          <h1>About Models</h1>
        </header>
        <span class="image main"
          ><img src="static/images/artificial-intelligence.webp" alt=""
        /></span>
        <p>Choose a model name for more information about it</p>
        <!-- Dropdown -->
        <div class="dropdown">
          <link
            href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css"
            rel="stylesheet"
            integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM"
            crossorigin="anonymous"
          />
          <button
            class="btn btn-secondary dropdown-toggle"
            type="button"
            data-bs-toggle="dropdown"
            aria-expanded="false"
          >
            --Select Model--
          </button>
          <ul class="dropdown-menu">
            <li>
              <a class="dropdown-item" href="/AboutModel/GPT-2">GPT-2</a>
            </li>
            <li>
              <a class="dropdown-item" href="/AboutModel/GPT-3.5">GPT-3.5</a>
            </li>
            <li>
              <a class="dropdown-item" href="/AboutModel/Bert-base"
                >Bert-base</a
              >
            </li>
            <li><a class="dropdown-item" href="/AboutModel/Bart">Bart</a></li>
            <li>
              <a class="dropdown-item" href="/AboutModel/dialGPT">DialoGPT</a>
            </li>
            <li>
              <a class="dropdown-item" href="/AboutModel/RoBERTa">RoBERTa</a>
            </li>
          </ul>
        </div>
        <link
          rel="stylesheet"
          href="{{ url_for('static', filename='css/style.css') }}"
        />
      </div>
    </section>
    <!-- Main -->
    <head>
      <style>
        table {
          border-collapse: collapse;
          width: 100%;
        }
        th {
          background-color: #f2f2f2;
          color: black;
          font-weight: bold;
          text-align: center;
          padding: 12px;
        }
        td {
          border: 1px solid #ddd;
          padding: 12px;
        }
        td p {
          font-size: 14px;
        }
      </style>
    </head>
    <body>
      <table>
        <tr>
          <th style="color: black"></th>
          <th style="color: black">Bart-base</th>
          <th style="color: black">Bert-base-uncased</th>
          <th style="color: black">dialoGPT</th>
          <th style="color: black">GPT2</th>
          <th style="color: black">GPT3</th>
          <th style="color: black">RoBERTa</th>
        </tr>
        <tr>
          <td style="font-weight: bold">When was it created?</td>
          <td>October 29, 2020</td>
          <td>March 11, 2020</td>
          <td>June 2021</td>
          <td>
            February 2019, trained on data that cuts off at the end of 2017.
          </td>
          <td>2022</td>
          <td>2019</td>
        </tr>
        <tr>
          <td style="font-weight: bold">Primary Users</td>
          <td>
            Developers or researchers who are trying to develop NLP models.
          </td>
          <td>AI researchers and practitioners.</td>
          <td>AI researchers and practitioners</td>
          <td>AI researchers and practitioners.</td>
          <td>Developers and consumers</td>
          <td>Developers and consumers</td>
        </tr>
        <tr>
          <td style="font-weight: bold">Uses</td>
          <td>
            <p>
              The raw model is useful for text infilling, but it is primarily
              useful when trained on a supervised dataset.
            </p>
          </td>
          <td>
            <p>
              Bert-Base is commonly used for Masked language modeling (MLM)
              which involves taking a sentence, the model randomly masks 15% of
              the words in the input then run the entire masked sentence through
              the model and has to predict the masked words. It is also used for
              Next sentence prediction (NSP) in which the model concatenates two
              masked sentences as inputs during pretraining. Sometimes they
              correspond to sentences that were next to each other in the
              original text, sometimes not. The model then has to predict if the
              two sentences were following each other or not.
            </p>
          </td>
          <td>
            <h4>Chatbots:</h4>
            <p>
              DialoGPT can be used to create chatbot systems that engage in
              text-based conversations with users. It generates responses based
              on the given context and can handle multi-turn conversations.
            </p>
            <h4>Virtual Assistants:</h4>
            <p>
              DialoGPT can serve as the core component of virtual assistants or
              voice-activated systems, providing natural and interactive
              responses to user queries and commands.
            </p>
          </td>
          <td>
            <p>
              This language model is used by researchers to better understand
              the behaviors, capabilities, biases, and constraints of
              large-scale generative language models. Additionally, it can be
              used for writing assistance such as grammar assistance,
              autocompletion (for normal prose or code). More uses include
              creative writing and art like exploring the generation of
              creative, fictional texts; aiding the creation of poetry and other
              literary art. Lastly, GPT-2 can be used for entertainment such as
              creation of games, chatbots, and amusing generations.
            </p>
          </td>
          <td>
            <p>
              Generally, the model is supposed to be used for research by
              developers. It is meant to be accessible to devs with minimal
              experience in AI development. Since this model and its successor
              GPT-3.5 are the backend to Chat-GPT, consumers have also
              interacted with it. The large breadth and openness of GPT-3 make
              it useful for many tasks such as essay writing, code writing,
              question answering, etc.
            </p>
          </td>
          <td>
            <p>
              RoBERTa can be used for masked language modeling (MLM) but is
              mainly used for downstream tasks in ML.
            </p>
            <p>
              RoBERTa is primarily aimed at being fine-tuned on tasks that use
              the whole sentence (potentially masked) to make decisions, such as
              sequence classification, token classification, or question
              answering.
            </p>
          </td>
        </tr>
        <tr>
          <td style="font-weight: bold">Training</td>
          <td>
            <p>
              Bart was specifically trained on Wikipedia (~2.5B words) and
              Google's BookCorpus (~800M words)
            </p>
          </td>
          <td>
            <p>
              The BERT model was pretrained on BookCorpus, a dataset consisting
              of 11,038 unpublished books and English Wikipedia (excluding
              lists, tables, and headers).
            </p>
          </td>
          <td>
            <p>
              The model is trained on 147M multi-turn dialogue from Reddit
              discussion threads dating from 2005-2017.
            </p>
          </td>
          <td>
            <p>
              This model was trained on WebText, a dataset consisting of the
              text contents of 45 million links posted by users of Reddit.
              WebText is made of data derived from outbound links from Reddit
              and does not consist of data taken directly from Reddit itself.
            </p>
          </td>
          <td>
            <p>
              GPT-3 was trained on a filtered version of the CommonCrawl dataset
              which contains text posted on the internet. It was also trained on
              the Webtext dataset and English Wikipedia.
            </p>
          </td>
          <td>
            <p>
              RoBERTa was trained on a large dataset of Wikipedia English text
              and also BookCorpus, a dataset consisting of 11,038 unpublished
              books and English Wikipedia.
            </p>
          </td>
        </tr>
        <tr>
          <td style="font-weight: bold">General Data</td>
          <td>
            <p>Number of Parameters: 139M</p>
            <p>Model Size: 2.5 GB</p>
          </td>
          <td>
            <p>Number of Parameters: 110M</p>
            <p>Number of Layers: 12</p>
            <p>Number of Heads: 12</p>
            <p>Model Size: 730 KB</p>
          </td>
          <td>
            <p>Number of Parameters: 345M</p>
            <p>Model Size: 774 MB</p>
          </td>
          <td>
            <p>Number of Parameters: 124 million</p>
            <p>Model Size: 481 MB</p>
          </td>
          <td>
            <p>Number of Parameters: 175B</p>
            <p>Model Size: 800GB - 2TB (estimate)</p>
          </td>
          <td>
            <p>Number of Parameters: 355M</p>
            <p>Number of Layers: 24</p>
            <p>Number of Heads: 16</p>
          </td>
        </tr>
      </table>
    </body>
  </div>
</html>
{% endblock %}
